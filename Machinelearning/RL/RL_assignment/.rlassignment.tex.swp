\documentclass[11pt]{article}
%Gummi|065|=)

\usepackage{array}
\usepackage{graphicx}
\usepackage{tgschola}
\usepackage{algorithmic}
\usepackage{amssymb}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{argmax}

\title{\textbf{SSU : Assignment 5} \\ \textbf{Windy world, Reinforcement learning}}
\author{\textbf{Teymur Azayev}}
\date{}


\begin{document}

\maketitle

\section{Overview}
This document is a short report for the fifth assignment from the subject. It includes a theoretical overview of the task, an explanation of the implementation, results and a short discussion.

\section{Task}
The task is to find an optimal policy function in the windy griworld environment. The task
is posed as a standard reinforcement learning problem for the MDP defined by the gridworld environment.
We will take a look at using a few different algorithms for finding the optimal policy, both directly and indirectly. 


\section{Value iteration}
We will first use a value iteration approach to finding the optimal Q-function (action value function). 
The algorithm converges in 683 iterations to the optimal Q-value, meaning the Q-matrix for which we get the maximal
reward out of our MDP, in this case -15. Below is the Q matrix for each of the actions in order and the policy plotted in the gridworld.

\includegraphics[width=11cm]{actionvaluepolicy.png}
\\
Action: Left
\[
\begin{bmatrix}
  -13. & -13. & -12. & -11. & -11. & -10. & -10. & -9. & -8. & -7.\\
  -14. & -13. & -12. & -12. & -11. & -11. & -10. & -9. & -9. & -6.\\
  -14. & -14. & -13. & -12. & -12. & -11. & -10. & -6. & -9. & -5.\\
  -15. & -14. & -13. & -13. & -12. & -12. & -10. & 0. & -7. & -5.\\
  -15. & -15. & -14. & -13. & -12. & -11. & 0. & -8. & -1. & -2.\\
  -14. & -14. & -13. & -12. & -10. & 0. & 0. & -7. & -1. & -1.\\
  -13. & -13. & -13. & -12. & 0. & 0. & 0. & 0. & -1. & -1.\\
\end{bmatrix} 
\]
Action: Right
\[
\begin{bmatrix}
  -12. & -12. & -11. & -11. & -10. & -10. & -9. & -8. & -7. & -7.\\
  -13. & -12. & -11. & -11. & -10. & -10. & -9. & -8. & -5. & -6.\\
  -14. & -13. & -12. & -11. & -11. & -10. & -9. & -6. & -5. & -5.\\
  -14. & -13. & -12. & -12. & -11. & -10. & -9. & 0. & -5. & -4.\\
  -15. & -14. & -13. & -12. & -11. & -10. & 0. & 0. & -3. & -2.\\
  -14. & -13. & -12. & -11. & -10. & 0. & 0. & 0. & 0. & -1.\\
  -12. & -12. & -11. & -10. & 0. & 0. & 0. & 0. & 0. & 0.\\
\end{bmatrix}
\]
Action: Up
\[
\begin{bmatrix}
  -12. & -11. & -12. & -11. & -11. & -11. & -9. & -9. & -7. & -7.\\
  -13. & -12. & -11. & -11. & -10. & -10. & -9. & -9. & -7. & -6.\\
  -14. & -13. & -12. & -11. & -11. & -10. & -10. & -8. & -5. & -6.\\
  -14. & -14. & -13. & -12. & -11. & -11. & -9. & 0. & -5. & -4.\\
  -15. & -14. & -13. & -12. & -12. & -11. & 0. & 0. & -5. & -2.\\
  -14. & -14. & -12. & -12. & -11. & 0. & 0. & 0. & 0. & -2.\\
  -12. & -12. & -12. & -11. & 0. & 0. & 0. & 0. & 0. & 0.\\
\end{bmatrix}
\]
Action: Down
\[
\begin{bmatrix}
  -12. & -12. & -11. & -12. & -10. & -10. & -10. & -8. & -8. & -6.\\
  -14. & -14. & -13. & -12. & -10. & -10. & -9. & -8. & -6. & -5.\\
  -13. & -13. & -12. & -12. & -12. & -10. & -10. & -6. & -4. & -4.\\
  -15. & -13. & -12. & -12. & -12. & -10. & -10. & 0. & -4. & -3.\\
  -14. & -14. & -13. & -12. & -10. & -10. & 0. & 0. & -2. & -2.\\
  -13. & -13. & -12. & -12. & -10. & 0. & 0. & 0. & 0. & -1.\\
  -12. & -12. & -12. & -10. & 0. & 0. & 0. & 0. & 0. & 0.\\
\end{bmatrix}
\]

\section{SARSA}



\section{Discussion}


\section{Conclusion}
This concludes the report on this task.


\end{document}
