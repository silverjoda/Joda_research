\documentclass[11pt]{article}
%Gummi|065|=)

\usepackage{array}
\usepackage{graphicx}
\usepackage{tgschola}
\usepackage{algorithmic}
\usepackage{amssymb}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{argmax}

\title{\textbf{SSU : Assignment 2} \\ \textbf{Image segmentation using clustering}}
\author{\textbf{Teymur Azayev}}
\date{}


\begin{document}

\maketitle

\section{Overview}

This document is a short report for the second assignment from the subject. It includes a theoretical overview of the task, the EM algorithm, including implementation and comparisons to a few baseline approaches.

\section{Task}
Given a small set of images $T^m \in\{x^1, ... , x^m\}$ we are asked to segment the images into background - foreground parts. This means clustering each pixel into one of the two categories. The image shapes are generated by a common shape model which is common across the whole dataset. The RGB color vectors are generated using the appearance model which is specific to each image and is conditioned on the global shape model.


\section{EM Algorithm}
As mentioned in the task description, each pixel is eventually generated by an appearance model which is conditioned on the shape model. \\ 

\begin{eqnarray}
p_{u_i,\theta^l}(x_i^l,s^i) &=& p_{u_i,\theta^l}(x_i^l|s_i) p_{u_i}(s_i) \\
&=& p_{u_i,\theta^l}(x_i^l|s_i) \frac{e^{u_is_i}}{1 + e^{u_is_i}}
\end{eqnarray}

Where the prior is given by the shape model which was provided. This further gives the likelihood of a single pixel
in a given image: \\

\begin{eqnarray}
p_{u_i,\theta^l}(x_i^l,s) &=& \sum_{s_i\in\{0,1\}}{p_{u_i,\theta^l}(x_i^l|s_i) p_{u_i}(s_i) } 
\end{eqnarray}

The above can be simply extended as a sum over all pixels to give us a likelihood of an image and then summed over all images to give us the log likelihood of the whole dataset. \\ 

\begin{eqnarray}
L(u, \theta) &=& \frac{1}{m} \prod_{l \in T^m} {\prod_{i \in D}{ \sum_{s_i\in\{0,1\}}{p_{u_i,\theta^l}(x_i^l,s^i)  } } }\\
\mathcal{L}(u, \theta) &=& \frac{1}{m} \sum_{l \in T^m} {\sum_{i \in D}{ \log \sum_{s_i\in\{0,1\}}{p_{u_i,\theta^l}(x_i^l,s^i)  } } }
\end{eqnarray}

By using the bound:

\begin{eqnarray}
\log \sum_{s_i \in \{0,1\}}{ p_{u_i,\theta^l}(x_i^l,s^i)} &\geq& \sum_{s_i \in \{0,1\}}{ \alpha(s_i | x_i^l) p_{u_i,\theta^l}(x_i^l,s^i)} - \\
\sum_{s_i \in \{0,1\}}{ \alpha(s_i | x_i^l)\log\alpha(s_i | x_i^l)}
\end{eqnarray}

We get the objective function for the EM algorithm as: 

\begin{eqnarray}
(u^*,a^*,\theta^*) &=& \argmax\limits_{u^*,a^*,\theta^*} \frac{1}{m} \sum_l^m{ \sum_{i \in D}{ \sum_{s_i \in\{0,1\}}   [\alpha(s_i | x_i^l) \log p_{u_i,\theta^l}(x_i^l,s^i) -  \alpha(s_i | x_i^l)\log\alpha(s_i | x_i^l)] }}
\end{eqnarray}

To derive the E-step for the EM algorithm we can rewrite the lower bound as follows:

\begin{eqnarray}
\log \sum_{s_i \in \{0,1\}}{ p_{u_i,\theta^l}(x_i^l,s^i)} &\geq& \sum_{s_i \in\{0,1\}} \alpha(s_i | x_i^l) 
log{\frac{p_{u_i,\theta^l}(x_i^l,s^i)}{\alpha(s_i | x_i^l) }} 
\end{eqnarray}

\begin{eqnarray}
\log \sum_{s_i \in \{0,1\}}{ \alpha(s_i | x_i^l)  \frac{ p_{u_i,\theta^l}(x_i^l,s^i)}{\alpha(s_i | x_i^l) }} &\geq& \sum_{s_i \in\{0,1\}} \alpha(s_i | x_i^l) 
log{\frac{p_{u_i,\theta^l}(x_i^l,s^i)}{\alpha(s_i | x_i^l) }} 
\end{eqnarray}

\begin{eqnarray}
f(E[X]) &\geq& E[fX]
\end{eqnarray}

Where 

\begin{eqnarray}
X &=& \frac{ p_{u_i,\theta^l}(x_i^l,s^i)}{\alpha(s_i | x_i^l)}
\end{eqnarray}

And the expectation denotes the sum over $s_i \in\{0,1\}$ of the product of $\alpha(s_i | x_i^l)$ and $X$. \\ 
We know that for the inequality to hold with equality the expectation of $X$ must equal to $X$ itself which 
means that 

\begin{eqnarray} 
\frac{ p_{u_i,\theta^l}(x_i^l,s^i)}{\alpha(s_i | x_i^l)} &=& const
\end{eqnarray}

This means that 

\begin{eqnarray}
\alpha(s_i | x_i^l) &\propto& p_{u_i,\theta^l}(x_i^l,s^i)
\end{eqnarray}

We also know that since $\alpha(s_i | x_i^l)$ is a probability distribution it should sum up to 1.

\begin{eqnarray}
\alpha(s_i | x_i^l) &=& \frac{ p_{u_i,\theta^l}(x_i^l,s^i)}{\sum_{s_i} p_{u_i,\theta^l}(x_i^l,s)}  
\end{eqnarray}

\begin{eqnarray}
\alpha(s_i | x_i^l) &=& \frac{ p_{u_i,\theta^l}(x_i^l,s^i)}{p_{u_i,\theta^l}(x_i^l)}  
\end{eqnarray}

\begin{eqnarray}
\alpha(s_i | x_i^l) &=& p_{u_i,\theta^l}(s^i | x_i^l)
\end{eqnarray}

We can see that the E-step is simply assigning the posterior to the hidden variables

For the M-step we will simply manipulate the objective function to get maximization functions for each variable separately. \\ 
For the $u$ variables we can expand the most inner sum and maximize over the individual images. This is because the parameter $u_i$ for a certain pixel is shared across the images but is different for each pixel. We get:

\begin{eqnarray}
u^* &=& \argmax\limits_{u^*} \frac{1}{m}\sum_{l=1}^m \alpha(s_i = 1 | x_i^l) \log (\frac{e^{u_i}}{1 + e^{u_i}}) + \alpha(s_i = 0 | x_i^l) \log (\frac{1}{1 + e^{u_i}})
\end{eqnarray}

\begin{eqnarray}
 &=& \argmax\limits_{u^*} \frac{1}{m}\sum_{l=1}^m \alpha(s_i = 1 | x_i^l)(u_i - \log (1 + e^{u_i})) - \alpha(s_i = 0 | x_i^l) \log (1 + e^{u_i})
\end{eqnarray}

\begin{eqnarray}
 &=& \argmax\limits_{u^*} \frac{1}{m}\sum_{l=1}^m \alpha(s_i = 1 | x_i^l)(u_i - \log (1 + e^{u_i})) - (1 - \alpha(s_i = 1 | x_i^l) \log (1 + e^{u_i})
\end{eqnarray}

\begin{eqnarray}
 &=& \argmax\limits_{u^*} \frac{1}{m}\sum_{l=1}^m \alpha(s_i = 1 | x_i^l)u_i - \log (1 + e^{u_i})
\end{eqnarray}

Where equation 21 is our final result. In the process we have ignored constant terms which will fall out of the process when deriving the equation. As for the concavity of equation 21, we can see that 
$\alpha(s_i = 1 | x_i^l)u_i$ is a linear function which leaves us to verify whether $log{1 + e^{u_i}}$
is also a concave/convex function.  We can prove it using one of the definitions of concavity: $f((\alpha - 1)x + \alpha y) >  (\alpha - 1)f(x) + \alpha f(y) $. Verifying one such point is enough to induce the condition across the whole domain only if the function $f$ is monotonic, which in our case it is. To avoid unneccessary cluter of basic calculus the verification of the condition is ommitted here and only the result is stated that the condition holds with the opposite equality, meaning that the function is convex. The whole equation is then concave and has a unique global maximum. \\


For parameters $\theta_1,\theta_0$ we do a similar manipulation. This time we maximize through all the pixels in an image. We do this separately for each image because these parameters are specific to each image.

\begin{eqnarray}
\theta_1^* &=& \argmax\limits_{\theta_1*} \sum_{ i \in D} \alpha(s_i = 1 | x_i^l) \log p_{\theta^l}(x_i^l|s^i = 1) p_{u_i}(x_i^l) 
\end{eqnarray}

\begin{eqnarray}
&=& \argmax\limits_{\theta_1*} \sum_{ i \in D} \alpha(s_i = 1 | x_i^l) \log p_{\theta^l}(x_i^l|s^i = 1)
\end{eqnarray}

Performing analogically for $\theta_0$ we get
\begin{eqnarray}
\theta_0^* &=& \argmax\limits_{\theta_0*} \sum_{ i \in D} \alpha(s_i = 0 | x_i^l) \log p_{\theta^l}(x_i^l|s^i = 0) p_{u_i}(x_i^l) 
\end{eqnarray}

\section{Conclusion}
This concludes the report on this task.


\end{document}
