=====================================================
====== Roadplan for diploma thesis experiments ======
=====================================================

-----------------------------------------------------

Q1 - Is there a difference between a learned reactive
and recurrent policy? If not, can we somehow force
an action heirarchy through auxilliary penalization
functions?

Exp1 - Implement and train reactive and recurrent
agents on 3 different mujoco environments in GYM.
(Use existing PPO implementation)
Compare and examine both policies: In particular
we are interested in a) generalization and robustness
of the two policies b) If the recurrent policy
learns an action heirarchy

Exp2 - Attempt to force an action heirarchy on the
recurrent network by adding gradually increasing
low-pass filters to the various layers. The final
action policy will be either be the last action or
a reactive network which takes in the actions from
all the layers as input and maps to an action int
the action space.

-----------------------------------------------------

Q2 - Can we learn a heirarchical model of an
agent/environment using NN?

Exp1 - Use quadcopter simulation.
and using a trained policy and/or user input to
attempt to learn a simple SxA -> S mapping.
In particular we are interested in how many samples
such a model takes and how well it generalizes.

Exp2 - Attempt to learn a heirarchical (recurrent)
model on the same environment as Exp1.

Exp3 - Learn simple navigation tasks on the quadcopter
using simple states and heirarchical states using
some q-learning variant. The idea is that is should
be much faster when using the heirarchical states.
Navigational tasks can be to navigate from hoverpoint
A to hoverpoint B while avoiding obstacles and staying
at a constant height. Reward can be how close quad
is to target. Actionspace is (roll,pitch,yaw,throttle)
continuous. States are (th,omega,psi) estimated
orientation + (X,Y,Z) calculated acceleration
in the world frame of reference.
-----------------------------------------------------

T1 - Study Pilco and its improvements. Attempt to
find an implementation in python (or implement it)
and use as baseline.

-----------------------------------------------------

T2 - Get familiar with TRPO and PPO (PPO is currently
SOA in training continuous action-space RL agents).
Run OpenAI implementation of PPO and get familiar
with the code.

-----------------------------------------------------
